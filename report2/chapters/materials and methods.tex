\section{Materials and Methods}

In medical image analysis, a typical workflow involves executing several sequential algorithmic steps, a process collectively referred to as a pipeline \cite{b8}. The pipeline in this study consists of six steps, namely pre-processing, registration, feature extraction, classification, post-processing and evaluation. We utilized 20 training sets and 10 test sets of T1 and T2 MRI scans, each accompanied by labels and brain masks for skull stripping. Corresponding affine transformations were provided, allowing us to bypass the registration step during Random Forest training.

\subsection{Pre-processing}

As an initial step, pre-processing functions to refine and standardize all images in a dataset. Some common steps include background removal, noise reduction, intensity normalization and resampling \cite{b9}.

Pre-processing ensures the refinement and standardization of all images for subsequent analysis. In our implementation, the pipeline began with intensity normalization, executed via Z-Score normalization. This method computed the mean and standard deviation of pixel intensities, standardizing them to a zero-mean, unit-variance distribution. This step mitigated inconsistencies across datasets.

Next, skull stripping was performed, which applied binary masks to remove non-brain regions. This ensured only relevant anatomical structures were retained.

Additionally, we introduced experimental noise using salt-and-pepper transformations. This step was not part of standard pre-processing but was incorporated to test model robustness under challenging conditions.

% Ex: For the machine-learning approach, we proceeded with a skull stripping. 
% Ex: As an additional experimental feature, we implemented a sort of reverse pre-processing and added salt & peppper noise to our images to evaluate our models on noisy data.

\subsection{Registration}

Registration is used to align multiple images to a common reference frame. When only rotations and translations are involved, it is referred to as a rigid transformation while, when scale and skew factors are also included, it is referred to as an affine transformation. Nonlinear, deformable transformations also exist \cite{b10}.
% Ex: In this study, we aligned the T1- and T2-weighted images. 
% Ex: Registration was also used while constructing the atlas but this is discussed in an ensuing section.

\subsection{Feature Extraction}

Any extractable characteristic or property that describes the underlying medical image and is used in the analysis is coined as a feature. Some examples of image features are intensity, shape, and texture information \cite{b8}. In this study, we focused on extracting features that encapsulate spatial, textural, and statistical properties from T1 and T2 MRI images, providing a comprehensive characterization of the data.

Textural features, derived from local intensity distributions, were used to capture patterns and variations within tissue regions. These included metrics such as mean intensity, variance, skewness, and entropy, which are particularly valuable for distinguishing between homogeneous and heterogeneous structures. These features enable the identification of subtle differences in tissue composition. Statistical sampling of voxel intensities provided additional insights by focusing on representative subsets of the data. By generating masks to ensure balanced sampling across different labels, we accounted for variations in tissue representation and minimized the impact of class imbalance. This approach not only improved the quality of the feature set but also enhanced the training process for machine learning models.

The combination of these features ensured a robust and multidimensional characterization of the images, facilitating effective downstream processing and analysis.

\subsection{Classification}

Classification is the core of the automated segmentation process, where the chosen machine learning algorithm determines the label for each voxel in the image \cite{b8}. In this study, a Random Forest classifier was utilized due to its robustness in handling high-dimensional data and its resistance to overfitting.

To optimize the classifier’s performance, a grid search was conducted to determine the best hyperparameters for both standard and noisy datasets (with added salt-and-pepper noise). Interestingly, the same optimal hyperparameters were identified for both conditions, indicating the classifier’s adaptability to different data qualities. The model achieved the best results with 700 decision trees, a maximum tree depth of 45, and a square root selection of features at each split.

The Random Forest was trained on features extracted from the pre-processed images, and its predictions on test data provided voxel-level probabilities for each label. These outputs formed the basis for the subsequent evaluation step.


\subsection{Post-processing}

In this study, post-processing was not applied as it was not necessary to evaluate our hypothesis. The results were directly assessed to determine the segmentation performance, ensuring that the findings reflected the core methodologies without additional modifications.

\# TODO: What did we do to ML results? What did we do to the atlas?
%Should we maybe just remove this subsection because we dont use any postprocessing?

\subsection{Evaluation}

The final step of the pipeline is to quantitatively assess its performance. Such a numeric score is a quick and easy way to compare and contrast different pipelines. Note that qualitative assessment of the resulting segmentation results is also very important to consider when deciding the clinically "better" result.

Dice scores quantify the segmentation quality for each tissue type (e.g., gray matter, white matter) \cite{b12}. Evaluator tools calculate subject-wise and aggregated statistics, including mean and standard deviation, across test samples. Results are logged and saved in timestamped directories for analysis.

\subsection{Construction of Atlas Labels}
A reference atlas serves as a common space for alignment. All provided training images were used for generating the atlas by registering the segmentation labels of each patient to the reference atlas using the given affine transformations.  The atlas is constructed by averaging all the labels of each patient in the training set and then assigning each voxel the label that occurs most frequently. Additionally, morphological operations, including median filtering and opening/closing, refine segmentation quality and eliminate small artifacts.




