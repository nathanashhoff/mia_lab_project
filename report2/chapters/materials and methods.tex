\section{Materials and Methods}

In medical image analysis, a typical workflow involves executing several sequential algorithmic steps, a process collectively referred to as a pipeline \cite{b8}. The pipeline in this study consists of six steps, namely pre-processing, registration, feature extraction, classification, post-processing and evaluation.

\subsection{Pre-processing}

As an initial step, pre-processing functions to refine and standardize all images in a dataset. Some common steps include background removal, noise reduction, intensity normalization and resampling \cite{b9}.

Pre-processing ensures the refinement and standardization of all images for subsequent analysis. In our implementation, the pipeline began with intensity normalization, executed via Z-Score normalization. This method computed the mean and standard deviation of pixel intensities, standardizing them to a zero-mean, unit-variance distribution. This step mitigated inconsistencies across datasets.

Next, skull stripping was performed, which applied binary masks to remove non-brain regions. This ensured only relevant anatomical structures were retained.

Additionally, we introduced experimental noise using salt-and-pepper transformations. This step was not part of standard pre-processing but was incorporated to test model robustness under challenging conditions.

% Ex: For the machine-learning approach, we proceeded with a skull stripping. 
% Ex: As an additional experimental feature, we implemented a sort of reverse pre-processing and added salt & peppper noise to our images to evaluate our models on noisy data.

\subsection{Registration}

Registration is used to align multiple images to a common reference frame. When only rotations and translations are involved, it is referred to as a rigid transformation while, when scale and skew factors are also included, it is referred to as an affine transformation. Nonlinear, deformable transformations also exist \cite{b10}.

\# TODO: describe what registration we used, for atlas-related I suggest we speak about it in other section
% Ex: In this study, we aligned the T1- and T2-weighted images. 
% Ex: Registration was also used while constructing the atlas but this is discussed in an ensuing section.

\subsection{Feature Extraction}

Any extractable characteristic or property that describes the underlying medical image and is used in the analysis is coined as a feature. Some examples of image features are intensity, shape, and texture information \cite{b8}. In this study, we focused on extracting features that capture spatial, textural, and statistical properties of the images.

Textural features, derived from local intensity distributions, were used to capture patterns and variations within tissue regions. These included metrics such as mean intensity, variance, skewness, and entropy, which are particularly valuable for distinguishing between homogeneous and heterogeneous structures. These features enable the identification of subtle differences in tissue composition. Statistical sampling of voxel intensities provided additional insights by focusing on representative subsets of the data. By generating masks to ensure balanced sampling across different labels, we accounted for variations in tissue representation and minimized the impact of class imbalance. This approach not only improved the quality of the feature set but also enhanced the training process for machine learning models.

The combination of these features ensured a robust and multidimensional characterization of the images, facilitating effective downstream processing and analysis.

\subsection{Classification}

Classification is the core of the automated segmentation process, where the chosen machine learning algorithm determines the label for each voxel in the image \cite{b8}. In this study, a Random Forest classifier was utilized due to its robustness in handling high-dimensional data and its resistance to overfitting.

To optimize the classifier’s performance, a grid search was conducted to determine the best hyperparameters for both standard and noisy datasets (with added salt-and-pepper noise). Interestingly, the same optimal hyperparameters were identified for both conditions, indicating the classifier’s adaptability to different data qualities. The model achieved the best results with 700 decision trees (\texttt{n\_estimators}), a maximum tree depth of 45 (\texttt{max\_depth}), and a square root selection of features at each split (\texttt{max\_features = sqrt}).

The Random Forest was trained on features extracted from the pre-processed images, and its predictions on test data provided voxel-level probabilities for each label. These outputs formed the basis for subsequent post-processing and evaluation steps, ensuring accurate and reliable segmentation results across varying data conditions.


\subsection{Post-processing}

With the predicted labels obtained, post-processing ensures that these segmentation results are cleaned, polished and improved. Some common examples are smoothing operations and morphological filters to remove noise, refine contours and fill in holes \cite{b11}.

\# TODO: What did we do to ML results? What did we do to the atlas?
%Should we maybe just remove this subsection because we dont use any postprocessing?

\subsection{Evaluation}

The final step of the pipeline is to quantitatively assess its performance. Such a numeric score is a quick and easy way to compare and contrast different pipelines. Note that qualitative assessment of the resulting segmentation results is also very important to consider when deciding the clinically "better" result.

Dice scores quantify the segmentation quality for each tissue type (e.g., gray matter, white matter) \cite{b12}. Evaluator tools calculate subject-wise and aggregated statistics, including mean and standard deviation, across test samples. Results are logged and saved in timestamped directories for analysis.

\subsection{Construction of Atlas Labels}
A reference atlas serves as a common space for alignment. Training images are divided into subsets (e.g., 10 per atlas), which are registered to the reference atlas using affine transformations. Within each subset, atlases are constructed by assigning each voxel the label that occurs most frequently across all images in the subset. This majority-vote strategy ensures consistent labeling across atlases. Additionally, morphological operations, including median filtering and opening/closing, refine segmentation quality and eliminate small artifacts.

To account for variability across atlases, Dice scores calculated during validation are used to weight atlas contributions. A weighted atlas is then constructed by aggregating label-specific contributions of individual atlases based on their performance.




